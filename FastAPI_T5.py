# ------------------------- OS ë° ë¡œê¹… ê´€ë ¨ ê¸°ë³¸ ëª¨ë“ˆ ì„í¬íŠ¸ -------------------------
import os
import logging
import time

# FastAPI ê´€ë ¨ ëª¨ë“ˆ
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# SlowAPI ì†ë„ ì œí•œ
from slowapi import Limiter
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬: MT5 ëª¨ë¸
from transformers import MT5Tokenizer, MT5ForConditionalGeneration
import torch

# ------------------------- ë¡œê·¸ ì„¤ì • -------------------------
LOG_DIR = "./logs"
os.makedirs(LOG_DIR, exist_ok=True)
log_file_path = os.path.join(LOG_DIR, "chat_server.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
    ]
)

# ------------------------- MT5 ëª¨ë¸ ë¡œë”© -------------------------
HF_MODEL_ID = "pleyel/chatbot_test"

logging.info("ğŸ”„ Hugging Face ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = MT5Tokenizer.from_pretrained(HF_MODEL_ID)
model = MT5ForConditionalGeneration.from_pretrained(HF_MODEL_ID)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()
logging.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# ------------------------- FastAPI ì´ˆê¸°í™” -------------------------
app = FastAPI()
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.exception_handler(RateLimitExceeded)
async def rate_limit_handler(request: Request, exc: RateLimitExceeded):
    logging.warning(f"â›” ì†ë„ ì œí•œ ì´ˆê³¼: {request.client.host}")
    return JSONResponse(status_code=429, content={"error": "Too Many Requests"})

# ------------------------- CORS ì„¤ì • -------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------- ìš”ì²­ ëª¨ë¸ -------------------------
class ChatRequest(BaseModel):
    message: str

@app.post("/chat")
@limiter.limit("5/10seconds")
async def chat(request: Request, body: ChatRequest):
    message = body.message.strip()
    if not message:
        return JSONResponse(status_code=400, content={"error": "ë©”ì‹œì§€ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."})

    logging.info(f"ğŸ“© ìš”ì²­ ìˆ˜ì‹ : {message}")

    try:
        inputs = tokenizer(
            message,
            return_tensors="pt",
            max_length=512,
            padding="max_length",
            truncation=True
        ).to(device)

        max_output_len = 64  # âœ… ì¶œë ¥ ê¸¸ì´ í•˜ë“œì½”ë”© ë˜ëŠ” ìƒë‹¨ì—ì„œ ë”°ë¡œ ì„¤ì •í•´ë„ OK

        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_length=max_output_len,
                do_sample=True,
                temperature=0.8,
                top_k=50,
                top_p=0.9,
                repetition_penalty=2.0,
                no_repeat_ngram_size=3
            )
        elapsed = time.time() - start_time

        answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        # ë°˜ë³µ ë¬¸ì¥ í•„í„°ë§
        tokens = answer.split()
        if len(tokens) > 6 and tokens[:3] == tokens[3:6]:
            answer = "ë‹µë³€ì´ ë°˜ë³µë˜ì–´ ì •í™•íˆ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."

        logging.info(f"âœ… ì‘ë‹µ ì™„ë£Œ ({elapsed:.2f}s): {answer[:60]}...")

        return {
            "response": answer,
            "time_taken": round(elapsed, 2),
            "model": HF_MODEL_ID
        }

    except Exception as e:
        logging.exception("âŒ ì—ëŸ¬ ë°œìƒ")
        return JSONResponse(status_code=500, content={"error": str(e)})

# ------------------------- ì„œë²„ ì‹¤í–‰ í•¨ìˆ˜ -------------------------
def run():
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ------------------------- ì§ì ‘ ì‹¤í–‰ ì‹œ -------------------------
if __name__ == "__main__":
    run()
