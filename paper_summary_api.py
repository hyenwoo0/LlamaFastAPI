import os
import requests
import time
import xml.etree.ElementTree as ET
from fastapi import FastAPI, Query
from typing import List
import nltk
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx
import numpy as np
import calendar

# âœ… NLTK punkt ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì„¤ì •
NLTK_DATA_PATH = os.path.expanduser("~/nltk_data")
nltk.data.path = [NLTK_DATA_PATH]  # ì‹œìŠ¤í…œ ê¸°ë³¸ ê²½ë¡œ ì œê±°

# punkt í† í¬ë‚˜ì´ì €ê°€ ì—†ë‹¤ë©´ ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt", download_dir=NLTK_DATA_PATH)

# âœ… FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI()


# âœ… ì´ˆë¡ ìš”ì•½ í•¨ìˆ˜: TextRank ë°©ì‹ ìš”ì•½ (TF-IDF + PageRank)
def summarize_abstract(abstract: str, num_sentences: int = 2) -> str:
    sentences = sent_tokenize(abstract)
    if len(sentences) < 2:
        return abstract
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)
    sim_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()
    graph = nx.from_numpy_array(sim_matrix)
    scores = nx.pagerank(graph)
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    summary = " ".join([s for _, s in ranked_sentences[:num_sentences]])
    return summary


# âœ… í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜: TF-IDF ê¸°ì¤€ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ
def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    if not text or len(text.strip()) < 10:
        return []
    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform([text])
    scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0])
    ranked = sorted(scores, key=lambda x: x[1], reverse=True)
    return [word for word, _ in ranked[:top_n]]


# âœ… PubMed ë…¼ë¬¸ ê²€ìƒ‰ ë° ì •ë³´ ìˆ˜ì§‘ í•¨ìˆ˜
def fetch_papers(query: str, retmax: int = 5) -> List[dict]:
    search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    search_params = {
        "db": "pubmed",
        "term": query,
        "retmax": retmax,
        "retmode": "json"
    }
    response = requests.get(search_url, params=search_params)
    id_list = response.json().get("esearchresult", {}).get("idlist", [])

    print("[ğŸ§ª] PMID list:", id_list)

    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    papers = []

    # ì›” ë¬¸ìì—´ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë§µ (ì˜ˆ: Oct â†’ 10)
    month_map = {name: f"{num:02d}" for num, name in enumerate(calendar.month_abbr) if name}

    for pmid in id_list:
        fetch_params = {
            "db": "pubmed",
            "id": pmid,
            "retmode": "xml"
        }
        try:
            fetch_response = requests.get(fetch_url, params=fetch_params)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.text)

            articles = root.findall("PubmedArticle")
            for article in articles:
                article_data = article.find(".//Article")

                # ì œëª©
                title_en = article_data.findtext("ArticleTitle", default="(No Title)")

                # ì´ˆë¡
                abstract = article_data.findtext(".//AbstractText", default="")

                # í•™ìˆ ì§€ ì´ë¦„
                journal = article_data.findtext(".//Journal/Title", default="í•™ìˆ ì§€ ì—†ìŒ")

                # ë°œí–‰ì¼ êµ¬ì„±
                pub_date_elem = article.find(".//PubDate")
                year = pub_date_elem.findtext("Year", "")
                month_raw = pub_date_elem.findtext("Month", "")
                day = pub_date_elem.findtext("Day", "01")
                month = month_map.get(month_raw[:3].capitalize(), "01") if month_raw else "01"
                pub_date = f"{year}-{month}-{day}" if year else "ë‚ ì§œ ì •ë³´ ì—†ìŒ"

                # ë…¼ë¬¸ ìœ í˜•
                article_type_list = article.findall(".//PublicationType")
                article_types = [a.text for a in article_type_list if a.text] or ["ìœ í˜• ì—†ìŒ"]

                # í˜ì´ì§€ ì •ë³´
                pages = article.findtext(".//Pagination/MedlinePgn", default="í˜ì´ì§€ ì •ë³´ ì—†ìŒ")

                # ì €ì ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
                author_list = article_data.findall(".//Author")
                authors = []
                for author in author_list:
                    last = author.findtext("LastName")
                    fore = author.findtext("ForeName")
                    if last and fore:
                        authors.append(f"{fore} {last}")
                authors_str = authors if authors else ["ì €ì ì •ë³´ ì—†ìŒ"]

                # ìš”ì•½ ë° í‚¤ì›Œë“œ ìƒì„±
                summary = summarize_abstract(abstract) if abstract else "No abstract available"
                keywords = extract_keywords(abstract) if abstract else []

                # ë…¼ë¬¸ ë§í¬ ìƒì„±
                link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"

                # ë…¼ë¬¸ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬
                papers.append({
                    "title_en": title_en,
                    "abstract": summary,
                    "authors": authors_str,
                    "year": year or "ì—°ë„ ì •ë³´ ì—†ìŒ",
                    "pub_date": pub_date,
                    "journal": journal,
                    "article_types": article_types,
                    "pages": pages,
                    "keywords": keywords,
                    "link": link
                })

                print(f"[âœ…] PMID {pmid} processed. Title: {title_en}")
                time.sleep(0.5)

        except Exception as e:
            print(f"[âš ï¸] Error fetching PMID {pmid}: {e}")
            continue

    return papers


# âœ… API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ (/papers?query=...)
@app.get("/papers")
def get_paper_cards(query: str = Query(..., description="ë…¼ë¬¸ ê²€ìƒ‰ í‚¤ì›Œë“œ")):
    try:
        papers = fetch_papers(query=query, retmax=5)
        return {"query": query, "papers": papers}
    except Exception as e:
        return {"error": str(e)}


# âœ… ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë¡œì»¬ ì‹¤í–‰ ì‹œ ì‚¬ìš©)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("paper_summary_api:app", host="0.0.0.0", port=8000, reload=True)
